{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Forward Propagation - Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import all library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oz0otAKgK6pZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Init activation function list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Xrz_kzsKKjL"
      },
      "outputs": [],
      "source": [
        "activation = [\n",
        "    'relu',\n",
        "    'sigmoid',\n",
        "    'linear',\n",
        "    'softmax',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Init all functionality class to build Forward Propagation and FFNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2HNg3_NQKQ73"
      },
      "outputs": [],
      "source": [
        "class ActivationFunction:\n",
        "    def __init__(self, activation_function):\n",
        "        if activation_function == 'sigmoid':\n",
        "            self.name = 'sigmoid'\n",
        "            self.function = lambda net: 1 / (1 + np.exp(-net))\n",
        "            self.error_term_output = lambda output, target : output * (1 - output) * (target - output)\n",
        "            self.error_term_hidden = lambda output, error_term_output, weights : output * (1 - output) * np.dot(error_term_output, weights.T)\n",
        "            self.loss = lambda output, target : np.sum((target - output) ** 2) / 2\n",
        "        elif activation_function == 'relu':\n",
        "            self.name = 'relu'\n",
        "            self.function = lambda net: np.maximum(0, net)\n",
        "            self.error_term_output = lambda output, target : np.where(output > 0, 1, 0) * (target - output)\n",
        "            self.error_term_hidden = lambda output, error_term_output, weights : np.where(output > 0, 1, 0) * np.dot(error_term_output, weights.T)\n",
        "            self.loss = lambda output, target : np.sum((target - output) ** 2) / 2\n",
        "        elif activation_function == 'linear':\n",
        "            self.name = 'linear'\n",
        "            self.function = lambda net: net\n",
        "            self.error_term_output = lambda output, target : target - output\n",
        "            self.error_term_hidden = lambda _, error_term_output, weights : np.dot(error_term_output, weights.T)\n",
        "            self.loss = lambda output, target : np.sum((target - output) ** 2) / 2\n",
        "        elif activation_function == 'softmax':\n",
        "            self.name = 'softmax'\n",
        "            self.function = lambda net: np.exp(net) / np.sum(np.exp(net))\n",
        "            self.error_term_output = lambda output, target : (target - output)\n",
        "            self.error_term_hidden = lambda _, error_term_output, weights : np.dot(error_term_output, weights.T)\n",
        "            self.loss = lambda output, target : self.softmax_loss(output, target)\n",
        "\n",
        "    def softmax_loss(self, output, target):\n",
        "        target_label = np.argmax(target, axis=1)\n",
        "        return -np.sum(np.log(output[np.arange(len(target_label)), target_label]))\n",
        "\n",
        "    def get_name(self):\n",
        "        return self.name\n",
        "    \n",
        "    def get_activation_function(self):\n",
        "        return self.function\n",
        "    \n",
        "    def get_error_term_output(self):\n",
        "        return self.error_term_output\n",
        "    \n",
        "    def get_error_term_hidden(self):\n",
        "        return self.error_term_hidden\n",
        "    \n",
        "    def get_loss(self):\n",
        "        return self.loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yKQX8-nXHuJ7"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, neuron: int, activation_function: str, weights: np.array, bias: np.array, out: bool = False):\n",
        "        self.neuron = neuron\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.out = out\n",
        "        if activation_function not in activation:\n",
        "            raise Exception('Invalid activation function')\n",
        "        else:\n",
        "            self.activation_function = ActivationFunction(activation_function)\n",
        "            self.function = self.activation_function.get_activation_function()\n",
        "            if out:\n",
        "                self.error_term = self.activation_function.get_error_term_output()\n",
        "            else:\n",
        "                self.error_term = self.activation_function.get_error_term_hidden()\n",
        "            self.loss = self.activation_function.get_loss()\n",
        "\n",
        "    def forward(self, input: np.array):\n",
        "        self.last_activation = self.function(np.dot(input, self.weights) + self.bias)\n",
        "        return self.last_activation\n",
        "\n",
        "    def update_weight(self, input: np.array, error_term: np.array, learning_rate: float):\n",
        "        self.weights += learning_rate * np.dot(input.T, error_term)\n",
        "        self.bias += learning_rate * error_term.sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6EqC5A7ZKqpv"
      },
      "outputs": [],
      "source": [
        "class FFNN:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.node_weights = []\n",
        "\n",
        "    def add_layer(self, layer: Layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input: np.array):\n",
        "        output = input\n",
        "        batch_count = len(output)\n",
        "        for i in range(batch_count):\n",
        "            self.node_weights.append([output[i]])\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "            for i in range(batch_count):\n",
        "                temp_nodes = output[i].tolist()\n",
        "                self.node_weights[i].append(temp_nodes)\n",
        "        return output\n",
        "\n",
        "    def fit(self, input: np.array, target: np.array, learning_rate: float, batch_size: int, max_iteration: int, error_threshold: float):\n",
        "        for iter in range(max_iteration):\n",
        "            total_err = 0\n",
        "            for i in range(0, len(input), batch_size):\n",
        "                input_batch = input[i:i+batch_size]\n",
        "                target_batch = target[i:i+batch_size]\n",
        "                output_batch = self.forward(input_batch)\n",
        "                error_batch = np.mean(self.layers[-1].loss(output_batch, target_batch))\n",
        "                total_err += error_batch\n",
        "\n",
        "                self.backward(input_batch, output_batch, target_batch, learning_rate)\n",
        "            total_err /= len(input) / batch_size\n",
        "            if np.abs(total_err) < error_threshold:\n",
        "                return \"error_threshold\"\n",
        "        return \"max_iteration\"\n",
        "\n",
        "    def backward(self, input : np.array, output: np.array, target: np.array, learning_rate: float):\n",
        "        first_input = input\n",
        "        for i in range(len(self.layers)-1, -1, -1):\n",
        "            layer = self.layers[i]\n",
        "            input = first_input if i == 0 else self.layers[i-1].last_activation\n",
        "            if i == len(self.layers)-1:\n",
        "                error_term = layer.error_term(output, target)\n",
        "                layer.update_weight(input, error_term, learning_rate)\n",
        "            else:\n",
        "                error_term_output = self.layers[i+1].error_term(self.layers[i+1].last_activation, target)\n",
        "                error_term = layer.error_term(self.layers[i].last_activation, error_term_output, self.layers[i+1].weights)\n",
        "                layer.update_weight(input, error_term, learning_rate)\n",
        "        self.node_weights = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetworkGraph:\n",
        "    def __init__(self):\n",
        "        self.graphs = nx.DiGraph()\n",
        "        self.node_labels = {}\n",
        "\n",
        "    def add_nodes_layer(self, nodes, layer):\n",
        "        self.graphs.add_nodes_from(nodes, layer=layer)\n",
        "\n",
        "    def add_edge(self, node_source, node_goal):\n",
        "        self.graphs.add_edge(node_source, node_goal)\n",
        "\n",
        "    def add_all_nodes(self, node_weights):\n",
        "\n",
        "        # Add input bias label\n",
        "        self.node_labels[0] = \"Input\"+str(0)+\": \"+str(1)\n",
        "\n",
        "        node_number = 1\n",
        "\n",
        "        self.add_nodes_layer([i for i in range(len(node_weights[0])+1)], 0)\n",
        "\n",
        "        # Add input label\n",
        "        for i, node_value in enumerate(node_weights[0]):\n",
        "            self.node_labels[node_number] = \"Input\"+str(i+1)+\": \"+str(node_value)\n",
        "            node_number += 1\n",
        "\n",
        "        # Add nodes for each layer with the subset key\n",
        "        for i in range(1, len(node_weights)-1):\n",
        "            self.node_labels[node_number] = \"H\"+str(i)+str(0)+\": \"+str(1)\n",
        "            self.add_nodes_layer([node_number+j for j in range(len(node_weights[i])+1)], i)\n",
        "            node_number += 1\n",
        "            for j in range(len(node_weights[i])):\n",
        "                self.node_labels[node_number] = \"H\"+str(i)+str(j+1)+\": \"+str(node_weights[i][j])\n",
        "                node_number += 1\n",
        "\n",
        "        self.add_nodes_layer([node_number+j for j in range(len(node_weights[-1]))], len(node_weights))\n",
        "\n",
        "        # Add output nodes\n",
        "        for i, node_value in enumerate(node_weights[-1]):\n",
        "            self.node_labels[node_number] = \"Output\"+str(i+1)+\": \"+str(node_value)\n",
        "            node_number += 1\n",
        "\n",
        "    def add_all_edges(self, node_weights):\n",
        "        number_of_prev_neuron = 0\n",
        "        curr_number_neuron = 0\n",
        "\n",
        "        # Add edge for each node of input layer and hidden layer\n",
        "        for layer_number in range(len(node_weights)-2):\n",
        "            curr_number_neuron += len(node_weights[layer_number])+1\n",
        "            for i in range(len(node_weights[layer_number])+1):\n",
        "                for j in range(len(node_weights[layer_number+1])):\n",
        "                    self.add_edge(number_of_prev_neuron, curr_number_neuron+j+1)\n",
        "                number_of_prev_neuron += 1\n",
        "\n",
        "        # Add edge for each node on output layer\n",
        "        curr_number_neuron += len(node_weights[-2])+1\n",
        "        for i in range(len(node_weights[-2])+1):\n",
        "            for j in range(len(node_weights[-1])):\n",
        "                self.add_edge(number_of_prev_neuron, curr_number_neuron+j)\n",
        "            number_of_prev_neuron += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for saving the model\n",
        "def saveModel(weights, layers, fileName):\n",
        "    case_dict = {'case': {'weights': weights, 'model': {'layers': layers}}}\n",
        "    with open(f\"../../Bagian-B/model/{fileName}_latest_weights_and_structures.json\", 'w') as outfile:\n",
        "        json.dump(case_dict, outfile, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for visualizing the model\n",
        "def visualizeModel(ffnn, weights):\n",
        "    # Initialize a directed graph for visualization\n",
        "    node_weights = ffnn.node_weights\n",
        "    batch_count = len(node_weights)\n",
        "    for batch_num in range(batch_count):\n",
        "        neural_network_graph = NeuralNetworkGraph()\n",
        "\n",
        "        neural_network_graph.add_all_nodes(node_weights[batch_num])\n",
        "\n",
        "        neural_network_graph.add_all_edges(node_weights[batch_num])\n",
        "\n",
        "        # Assuming neural_network_graph.graphs is your graph object\n",
        "        neural_network_graphs = neural_network_graph.graphs  # Assuming this is a correct reference\n",
        "\n",
        "        # Add 'layer' attribute to nodes if it's missing\n",
        "        for node in neural_network_graphs.nodes():\n",
        "            if 'layer' not in neural_network_graphs.nodes[node]:\n",
        "                neural_network_graphs.nodes[node]['layer'] = 0  # Set a default layer if needed\n",
        "\n",
        "        # Plot the neural network structure\n",
        "        pos = nx.multipartite_layout(neural_network_graphs, subset_key=\"layer\", align='horizontal')\n",
        "        nx.draw(neural_network_graphs, pos, with_labels=True, labels=neural_network_graph.node_labels, node_size=2000, node_color=\"lightblue\", font_size=7, font_weight=\"bold\")\n",
        "\n",
        "        # Add edge labels for better understanding\n",
        "        edge_labels = {}\n",
        "        index = 0\n",
        "        sub_index = 0\n",
        "        sub_sub_index = 0\n",
        "        for u, v in neural_network_graphs.edges():\n",
        "            edge_labels[(u,v)] = \"W: \"+str(weights[index][sub_index][sub_sub_index])\n",
        "            if(sub_sub_index+1<len(weights[index][sub_index])):\n",
        "                sub_sub_index += 1\n",
        "            else:\n",
        "                sub_sub_index = 0\n",
        "                if(sub_index+1<len(weights[index])):\n",
        "                    sub_index += 1\n",
        "                else:\n",
        "                    sub_index = 0\n",
        "                    index += 1\n",
        "        print(\"==========================================================================================================================================================================================================================\")\n",
        "        print(\"Edge labels: \",edge_labels)\n",
        "        print(\"==========================================================================================================================================================================================================================\")\n",
        "        nx.draw_networkx_edge_labels(neural_network_graphs, pos, edge_labels=edge_labels, font_size=7, font_color='red')\n",
        "\n",
        "        plt.title(\"Neural Network Structure (Input to Output)\")\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# without using saved model (contains weights and structure)\n",
        "def calculateWithoutSavedModel(fileName):\n",
        "    model = open(f'../../Bagian-A/test/{fileName}.json', 'r')\n",
        "    model = json.load(model)\n",
        "\n",
        "    layers = model['case']['model']['layers']\n",
        "    weights = model['case']['weights']\n",
        "\n",
        "    # for saving model\n",
        "    layers_dict = []\n",
        "    weights_dict = []\n",
        "\n",
        "    ffnn = FFNN()\n",
        "    for i in range (len(layers)):\n",
        "        layer = layers[i]\n",
        "        weight = weights[i]\n",
        "\n",
        "        # add the layers and weights\n",
        "        layers_dict.append(layer)\n",
        "        weights_dict.append(weight)\n",
        "\n",
        "        ffnn.add_layer(Layer(layer[\"number_of_neurons\"], layer[\"activation_function\"], np.array(weight[1:]), np.array(weight[0])))\n",
        "\n",
        "    saveModel(weights_dict, layers_dict, fileName) # save the model\n",
        "\n",
        "    input = model[\"case\"][\"input\"]\n",
        "\n",
        "    output = ffnn.forward(input).tolist()\n",
        "    expected_output = model['expect']['output']\n",
        "    print(\"==========================================================================================================================================================================================================================\")\n",
        "    print(\"Node values: \", ffnn.node_bobots)\n",
        "\n",
        "    print(f'output: {output}')\n",
        "    print(f'expected output: {expected_output}')\n",
        "    print(\"==========================================================================================================================================================================================================================\")\n",
        "\n",
        "    visualizeModel(ffnn, weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with saved model\n",
        "def calculateWithSavedModel(fileName):\n",
        "    model = open(f'../../Bagian-A/test/{fileName}.json', 'r')\n",
        "    model = json.load(model)\n",
        "\n",
        "    # loading the saved models (it contains the layers and weights)\n",
        "    savedModel = open(f'../../Bagian-A/model/{fileName}_latest_weights_and_structures.json', 'r')\n",
        "    savedModel = json.load(savedModel)\n",
        "\n",
        "    layers = savedModel['case']['model']['layers']\n",
        "    weights = savedModel['case']['weights']\n",
        "\n",
        "    ffnn = FFNN()\n",
        "    for i in range (len(layers)):\n",
        "        layer = layers[i]\n",
        "        weight = weights[i]\n",
        "        ffnn.add_layer(Layer(layer[\"number_of_neurons\"], layer[\"activation_function\"], np.array(weight[1:]), np.array(weight[0])))\n",
        "\n",
        "    input = model[\"case\"][\"input\"]\n",
        "\n",
        "    output = ffnn.forward(input).tolist()\n",
        "    expected_output = model['expect']['output']\n",
        "    print(\"==========================================================================================================================================================================================================================\")\n",
        "    print(\"Node values: \", ffnn.node_weights)\n",
        "    print(f'output: {output}')\n",
        "    print(f'expected output: {expected_output}')\n",
        "    print(\"==========================================================================================================================================================================================================================\")\n",
        "\n",
        "    visualizeModel(ffnn, weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# region BAGIAN B\n",
        "BAGIAN_B_TEST_CASES = {\n",
        "    '1': '../../Bagian-B/test/linear_small_lr.json',\n",
        "    '2': '../../Bagian-B/test/linear_two_iteration.json',\n",
        "    '3': '../../Bagian-B/test/linear.json',\n",
        "    '4': '../../Bagian-B/test/mlp.json',\n",
        "    '5': '../../Bagian-B/test/relu_b.json',\n",
        "    '6': '../../Bagian-B/test/softmax.json',\n",
        "    '7': '../../Bagian-B/test/softmax_two_layer.json',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start_test_case_b(test_case):\n",
        "    model = open(test_case, 'r')\n",
        "    model = json.load(model)\n",
        "\n",
        "    layers = model['case']['model']['layers']\n",
        "    init_weights = model['case']['initial_weights']\n",
        "\n",
        "    ffnn = FFNN()\n",
        "    for i in range (len(layers)):\n",
        "        layer = layers[i]\n",
        "        weight = init_weights[i]\n",
        "        if (i == len(layers) - 1):\n",
        "            ffnn.add_layer(Layer(layer[\"number_of_neurons\"], layer[\"activation_function\"], np.array(weight[1:]), np.array(weight[0]), out=True))\n",
        "        else:\n",
        "            ffnn.add_layer(Layer(layer[\"number_of_neurons\"], layer[\"activation_function\"], np.array(weight[1:]), np.array(weight[0])))\n",
        "\n",
        "\n",
        "    input = np.array(model[\"case\"][\"input\"])\n",
        "    target = np.array(model[\"case\"][\"target\"])\n",
        "\n",
        "    params = model[\"case\"][\"learning_parameters\"]\n",
        "\n",
        "    stop_cond = ffnn.fit(input, target, params[\"learning_rate\"], params[\"batch_size\"], params[\"max_iteration\"], params[\"error_threshold\"])\n",
        "\n",
        "    excepted_weights = model[\"expect\"][\"final_weights\"]\n",
        "    excepted_stop_cond = model[\"expect\"][\"stopped_by\"]\n",
        "\n",
        "    print(f'\\nResults ------------------')\n",
        "    print(f'Stop condition: {stop_cond}')\n",
        "    print(f'Expected stop condition: {excepted_stop_cond}')\n",
        "\n",
        "    for i in range(len(ffnn.layers)):\n",
        "        print(f'Layer {i+1} weights:')\n",
        "        bias_weights = np.array([ffnn.layers[i].bias] + ffnn.layers[i].weights.tolist())\n",
        "        print(bias_weights)\n",
        "        print(f'Expected Layer {i+1} weights:')\n",
        "        print(np.array(excepted_weights[i]))\n",
        "    print()\n",
        "    visualizeModel(ffnn, init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_backward(test_case):\n",
        "\n",
        "    l = len(BAGIAN_B_TEST_CASES) + 1\n",
        "    print(f\"Test Case: \", BAGIAN_B_TEST_CASES[test_case])\n",
        "\n",
        "    if test_case == str(l):\n",
        "        test_case = input(\"Input test file path: \")\n",
        "        start_test_case_b(test_case)\n",
        "        return\n",
        "    \n",
        "    start_test_case_b(BAGIAN_B_TEST_CASES[test_case])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kasus Uji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear_small_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case:  ../../Bagian-B/test/linear_small_lr.json\n",
            "\n",
            "Results ------------------\n",
            "Stop condition: max_iteration\n",
            "Expected stop condition: max_iteration\n",
            "Layer 1 weights:\n",
            "[[ 0.1012  0.3006  0.1991]\n",
            " [ 0.4024  0.201  -0.7019]\n",
            " [ 0.1018 -0.799   0.4987]]\n",
            "Expected Layer 1 weights:\n",
            "[[ 0.1008  0.3006  0.1991]\n",
            " [ 0.402   0.201  -0.7019]\n",
            " [ 0.101  -0.799   0.4987]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_backward(\"1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## linear_two_iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case:  ../../Bagian-B/test/linear_two_iteration.json\n",
            "\n",
            "Results ------------------\n",
            "Stop condition: max_iteration\n",
            "Expected stop condition: max_iteration\n",
            "Layer 1 weights:\n",
            "[[ 0.166  0.338  0.153]\n",
            " [ 0.502  0.226 -0.789]\n",
            " [ 0.214 -0.718  0.427]]\n",
            "Expected Layer 1 weights:\n",
            "[[ 0.166  0.338  0.153]\n",
            " [ 0.502  0.226 -0.789]\n",
            " [ 0.214 -0.718  0.427]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_backward(\"2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case:  ../../Bagian-B/test/linear.json\n",
            "\n",
            "Results ------------------\n",
            "Stop condition: max_iteration\n",
            "Expected stop condition: max_iteration\n",
            "Layer 1 weights:\n",
            "[[ 0.22  0.36  0.11]\n",
            " [ 0.64  0.3  -0.89]\n",
            " [ 0.28 -0.7   0.37]]\n",
            "Expected Layer 1 weights:\n",
            "[[ 0.22  0.36  0.11]\n",
            " [ 0.64  0.3  -0.89]\n",
            " [ 0.28 -0.7   0.37]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_backward(\"3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case:  ../../Bagian-B/test/mlp.json\n",
            "\n",
            "Results ------------------\n",
            "Stop condition: max_iteration\n",
            "Expected stop condition: max_iteration\n",
            "Layer 1 weights:\n",
            "[[ 0.08581778  0.32009219]\n",
            " [-0.34196319  0.46252925]\n",
            " [ 0.45330896  0.441397  ]]\n",
            "Expected Layer 1 weights:\n",
            "[[ 0.08592   0.32276 ]\n",
            " [-0.33872   0.46172 ]\n",
            " [ 0.449984  0.440072]]\n",
            "Layer 2 weights:\n",
            "[[ 0.2748    0.188   ]\n",
            " [ 0.435904 -0.53168 ]\n",
            " [ 0.68504   0.7824  ]]\n",
            "Expected Layer 2 weights:\n",
            "[[ 0.2748    0.188   ]\n",
            " [ 0.435904 -0.53168 ]\n",
            " [ 0.68504   0.7824  ]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_backward(\"4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## relu_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case:  ../../Bagian-B/test/relu_b.json\n",
            "\n",
            "Results ------------------\n",
            "Stop condition: max_iteration\n",
            "Expected stop condition: max_iteration\n",
            "Layer 1 weights:\n",
            "[[-0.211   0.105   0.885 ]\n",
            " [ 0.3033  0.5285  0.3005]\n",
            " [-0.489  -0.905   0.291 ]]\n",
            "Expected Layer 1 weights:\n",
            "[[-0.211   0.105   0.885 ]\n",
            " [ 0.3033  0.5285  0.3005]\n",
            " [-0.489  -0.905   0.291 ]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_backward(\"5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case:  ../../Bagian-B/test/softmax.json\n",
            "\n",
            "Results ------------------\n",
            "Stop condition: max_iteration\n",
            "Expected stop condition: max_iteration\n",
            "Layer 1 weights:\n",
            "[[ 0.12674605  0.9149538  -0.14169985]\n",
            " [-0.33551647  0.67700488  0.45851159]\n",
            " [ 0.48314436 -0.85241216  0.2692678 ]\n",
            " [ 0.3400255   0.57237542 -0.31240092]\n",
            " [ 0.31397716  0.46349737  0.72252547]\n",
            " [-0.69652442  0.4789189   0.61760552]\n",
            " [-0.50884515 -0.36354141  0.57238656]\n",
            " [ 0.41891295  0.26354517 -0.48245812]\n",
            " [ 0.90374164 -0.01759501 -0.08614663]]\n",
            "Expected Layer 1 weights:\n",
            "[[ 0.12674605  0.9149538  -0.14169985]\n",
            " [-0.33551647  0.67700488  0.45851159]\n",
            " [ 0.48314436 -0.85241216  0.2692678 ]\n",
            " [ 0.3400255   0.57237542 -0.31240092]\n",
            " [ 0.31397716  0.46349737  0.72252547]\n",
            " [-0.69652442  0.4789189   0.61760552]\n",
            " [-0.50884515 -0.36354141  0.57238656]\n",
            " [ 0.41891295  0.26354517 -0.48245812]\n",
            " [ 0.90374164 -0.01759501 -0.08614663]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_backward(\"6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## softmax_two_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case:  ../../Bagian-B/test/softmax_two_layer.json\n",
            "\n",
            "Results ------------------\n",
            "Stop condition: error_threshold\n",
            "Expected stop condition: error_threshold\n",
            "Layer 1 weights:\n",
            "[[-0.11927308  0.01473058 -0.47644775  0.42651942]\n",
            " [-0.70465145 -1.3477555  -1.56195381  0.7022227 ]\n",
            " [-0.4034573   1.65264803 -0.98739569 -1.31705963]]\n",
            "Expected Layer 1 weights:\n",
            "[[-0.28730211 -0.28822282 -0.70597451  0.42094471]\n",
            " [-0.5790794  -1.1836444  -1.34287961  0.69575311]\n",
            " [-0.41434377  1.51314676 -0.97649086 -1.3043465 ]]\n",
            "Layer 2 weights:\n",
            "[[-1.71196389  1.73196389]\n",
            " [-0.46000472  0.44000472]\n",
            " [ 1.22762769 -1.20762769]\n",
            " [-1.1009413   1.0809413 ]\n",
            " [ 1.07528479 -1.05528479]]\n",
            "Expected Layer 2 weights:\n",
            "[[-1.72078607  1.74078607]\n",
            " [-0.50352956  0.48352956]\n",
            " [ 1.25764816 -1.23764816]\n",
            " [-1.16998784  1.14998784]\n",
            " [ 1.0907634  -1.0707634 ]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_backward(\"7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP sklearn pada iris.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value comparison:\n",
            "Predicted values, Real values\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-setosa\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-versicolor\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Iris-versicolor , Iris-virginica\n",
            "Accuracy: 0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "data = pd.read_csv('../test/iris.csv')\n",
        "training = data.drop(columns=['Species', 'Id'])\n",
        "target = data['Species']\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(4, 9, 3), max_iter=1000, activation='logistic', learning_rate='constant', learning_rate_init=0.01)\n",
        "mlp.fit(training, target)\n",
        "\n",
        "target_predicted = mlp.predict(training)\n",
        "\n",
        "print(\"Value comparison:\")\n",
        "print(\"Predicted values, Real values\")\n",
        "for i in range(len(target_predicted)):\n",
        "    print(target_predicted[i], \",\", target[i])\n",
        "print(\"Accuracy:\", mlp.score(training, target))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
